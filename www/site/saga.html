<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>saga - Text tokenization for OCaml | raven</title>
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <main class="main-content">
    <nav class="saga-nav nav-breadcrumb">
      <a href="/">raven</a> / saga
      [ <a href="/docs/saga/">docs</a> |
      <a href="https://github.com/raven-ml/raven/tree/main/saga">source</a> ]
    </nav>

    <div class="saga-hero hero">
      <h1>saga</h1>
      <p class="tagline">Hugging Face quality. OCaml efficiency. Production-ready tokenization.</p>
    </div>

    <hr>

    <h2>why saga?</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-teal">modern tokenizers</h3>
        <p>BPE and WordPiece out of the box. Compatible with pretrained models from Hugging Face.</p>
      </div>
      <div>
        <h3 class="color-teal">unicode done right</h3>
        <p>Proper handling of multilingual text, emoji, and special characters. No surprises.</p>
      </div>
      <div>
        <h3 class="color-teal">direct tensor encoding</h3>
        <p>Encode straight to Nx tensors. Batch processing with padding and truncation built in.</p>
      </div>
      <div>
        <h3 class="color-teal">type-safe vocabularies</h3>
        <p>Vocabularies that can't go out of sync. Special tokens that always exist.</p>
      </div>
    </div>

    <hr>

    <h2>show me the code</h2>

    <div class="code-compare">
      <div>
        <h4>PYTHON</h4>
        <pre><span class="keyword">from</span> <span class="type">transformers</span> <span class="keyword">import</span> <span class="type">AutoTokenizer</span>

<span class="comment"># Load tokenizer</span>
<span class="keyword">tokenizer</span> <span class="operator">=</span> <span class="type">AutoTokenizer</span>.<span class="function">from_pretrained</span>(
    <span class="string">"bert-base"</span>
)

<span class="comment"># Tokenize</span>
<span class="keyword">tokens</span> <span class="operator">=</span> <span class="keyword">tokenizer</span>.<span class="function">tokenize</span>(<span class="string">"Hello world"</span>)

<span class="comment"># Encode to IDs</span>
<span class="keyword">input_ids</span> <span class="operator">=</span> <span class="keyword">tokenizer</span>.<span class="function">encode</span>(
    <span class="string">"Hello world"</span>,
    <span class="keyword">max_length</span><span class="operator">=</span><span class="number">128</span>,
    <span class="keyword">padding</span><span class="operator">=</span><span class="string">"max_length"</span>
)

<span class="comment"># Batch encode</span>
<span class="keyword">batch</span> <span class="operator">=</span> <span class="keyword">tokenizer</span>(
    [<span class="string">"Hello"</span>, <span class="string">"World"</span>],
    <span class="keyword">padding</span><span class="operator">=</span><span class="keyword">True</span>,
    <span class="keyword">return_tensors</span><span class="operator">=</span><span class="string">"pt"</span>
)</pre>
      </div>
      <div>
        <h4>SAGA</h4>
        <pre><span class="keyword">open</span> <span class="type">Saga</span>

<span class="comment">(* Load tokenizer *)</span>
<span class="keyword">let</span> <span class="keyword">tokenizer</span> <span class="operator">=</span> 
  <span class="type">Wordpiece</span>.<span class="function">from_files</span> 
    <span class="operator">~</span><span class="keyword">vocab</span><span class="operator">:</span><span class="string">"vocab.txt"</span>

<span class="comment">(* Tokenize *)</span>
<span class="keyword">let</span> <span class="keyword">tokens</span> <span class="operator">=</span> <span class="type">Wordpiece</span>.<span class="function">tokenize</span> <span class="keyword">tokenizer</span> <span class="string">"Hello world"</span>

<span class="comment">(* Encode to IDs *)</span>
<span class="keyword">let</span> <span class="keyword">input_ids</span> <span class="operator">=</span> 
  <span class="function">encode</span> <span class="operator">~</span><span class="keyword">vocab</span> 
    <span class="operator">~</span><span class="keyword">max_len</span><span class="operator">:</span><span class="number">128</span> 
    <span class="string">"Hello world"</span>

<span class="comment">(* Batch encode - returns Nx tensor *)</span>
<span class="keyword">let</span> <span class="keyword">batch</span> <span class="operator">=</span> 
  <span class="function">encode_batch</span> <span class="operator">~</span><span class="keyword">vocab</span>
    <span class="operator">~</span><span class="keyword">pad</span><span class="operator">:</span><span class="keyword">true</span>
    [<span class="string">"Hello"</span>; <span class="string">"World"</span>]</pre>
      </div>
    </div>

    <hr>

    <h2>tokenization that works</h2>

    <pre><span class="comment">(* Simple tokenization *)</span>
<span class="keyword">let</span> <span class="keyword">tokens</span> <span class="operator">=</span> <span class="function">tokenize</span> <span class="string">"Hello world! 你好世界"</span>
<span class="comment">(* ["Hello"; "world"; "!"; "你好世界"] *)</span>

<span class="comment">(* Build vocabulary from corpus *)</span>
<span class="keyword">let</span> <span class="keyword">vocab</span> <span class="operator">=</span> 
  <span class="function">vocab</span> <span class="operator">~</span><span class="keyword">max_size</span><span class="operator">:</span><span class="number">30000</span> <span class="operator">~</span><span class="keyword">min_freq</span><span class="operator">:</span><span class="number">2</span>
    (<span class="type">List</span>.<span class="function">concat_map</span> <span class="function">tokenize</span> <span class="keyword">corpus</span>)

<span class="comment">(* BPE tokenization *)</span>
<span class="keyword">let</span> <span class="keyword">bpe</span> <span class="operator">=</span> <span class="type">Bpe</span>.<span class="function">from_files</span> <span class="operator">~</span><span class="keyword">vocab</span><span class="operator">:</span><span class="string">"vocab.json"</span> <span class="operator">~</span><span class="keyword">merges</span><span class="operator">:</span><span class="string">"merges.txt"</span> <span class="keyword">in</span>
<span class="keyword">let</span> <span class="keyword">tokens</span> <span class="operator">=</span> <span class="type">Bpe</span>.<span class="function">tokenize</span> <span class="keyword">bpe</span> <span class="string">"unrecognizable"</span>
<span class="comment">(* ["un", "##rec", "##ogn", "##iz", "##able"] *)</span>

<span class="comment">(* Unicode normalization *)</span>
<span class="keyword">let</span> <span class="keyword">clean</span> <span class="operator">=</span> 
  <span class="function">normalize</span> <span class="operator">~</span><span class="keyword">lowercase</span><span class="operator">:</span><span class="keyword">true</span> <span class="operator">~</span><span class="keyword">strip_accents</span><span class="operator">:</span><span class="keyword">true</span> 
    <span class="string">"Café RÉSUMÉ"</span>
<span class="comment">(* "cafe resume" *)</span></pre>

    <hr>

    <h2>the good parts</h2>

    <p><b>Pretrained compatibility</b><br>
    Load vocabularies from Hugging Face models. Your BERT tokenizer works out of the box.</p>

    <p><b>Batch processing that scales</b><br>
    Encode thousands of texts efficiently. Automatic padding and truncation. Direct tensor output.</p>

    <p><b>Unicode that doesn't break</b><br>
    Proper grapheme clustering. CJK text handled correctly. Emoji that don't corrupt your tokens.</p>

    <p><b>Type safety throughout</b><br>
    Vocabularies that can't get out of sync. Special tokens that are always defined. No string keys to typo.</p>

    <hr>

    <h2>what's implemented</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-teal">tokenizers</h3>
        <ul>
          <li>✓ Word-level</li>
          <li>✓ Character-level</li>
          <li>✓ BPE</li>
          <li>✓ WordPiece</li>
          <li>○ SentencePiece</li>
        </ul>
      </div>
      <div>
        <h3 class="color-teal">preprocessing</h3>
        <ul>
          <li>✓ Unicode normalization</li>
          <li>✓ Accent stripping</li>
          <li>✓ Case folding</li>
          <li>✓ Whitespace cleanup</li>
          <li>✓ Control char removal</li>
        </ul>
      </div>
      <div>
        <h3 class="color-teal">features</h3>
        <ul>
          <li>✓ Vocabulary management</li>
          <li>✓ Special tokens</li>
          <li>✓ Batch encoding</li>
          <li>✓ Padding/truncation</li>
          <li>○ Training tokenizers</li>
        </ul>
      </div>
      <div>
        <h3 class="color-teal">integration</h3>
        <ul>
          <li>✓ Nx tensor output</li>
          <li>✓ File I/O</li>
          <li>✓ Token caching</li>
          <li>○ Rust backend</li>
          <li>○ Model zoo</li>
        </ul>
      </div>
    </div>

    <hr>

    <h2>get started</h2>

    <p>Saga is part of the Raven ecosystem. When it's released:</p>

    <pre>
<span class="comment"># Install</span>
<span class="function">opam</span> install saga

<span class="comment"># Try it</span>
<span class="keyword">open</span> <span class="type">Saga</span>

<span class="keyword">let</span> () <span class="operator">=</span>
  <span class="keyword">let</span> <span class="keyword">text</span> <span class="operator">=</span> <span class="string">"Hello world! How are you?"</span> <span class="keyword">in</span>
  <span class="keyword">let</span> <span class="keyword">tokens</span> <span class="operator">=</span> <span class="function">tokenize</span> <span class="keyword">text</span> <span class="keyword">in</span>
  <span class="type">List</span>.<span class="function">iter</span> <span class="function">print_endline</span> <span class="keyword">tokens</span>
    </pre>

    <p>For now, check out the <a href="/docs/saga/">documentation</a> to learn more.</p>
  </main>
</body>
</html>